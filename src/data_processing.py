# src/data_processing.py
import re
import json
from pathlib import Path

from config import DATA_RAW_DIR, DATA_PROCESSED_DIR


def extract_type_from_folder(folder_name: str) -> str:
    """ä»æ–‡ä»¶å¤¹åç§°æå–æ³•å¾‹ç±»å‹"""
    if '-' in folder_name:
        return folder_name.split('-', 1)[1]
    return folder_name


def extract_law_name_from_file(file_name: str) -> str:
    """ä»æ–‡ä»¶åæå–æ³•å¾‹åç§°ï¼Œå»é™¤æ‹¬å·å†…å®¹"""
    name = Path(file_name).stem
    pattern = r'\s*\([^)]+\)$'
    return re.sub(pattern, '', name)


def parse_law_content(content: str):
    """
    æŒ‰æ¡æ¬¾åˆ‡åˆ†æ³•å¾‹å†…å®¹ï¼Œæ¯æ¡è®°å½•å¯¹åº”ä¸€ä¸ªæ³•å¾‹æ¡æ¬¾
    """
    lines = content.split('\n')
    results = []

    current_chapter = "ç¬¬ä¸€ç«  æ€»åˆ™"
    current_article = ""
    current_content = []
    article_number = 0

    for line in lines:
        line = line.strip()

        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šæ ‡è®°
        if not line or line == "<!-- INFO END -->":
            continue

        # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
        if line.startswith('## '):
            current_chapter = line[3:].strip()
            # ç« èŠ‚æ ‡é¢˜å•ç‹¬ä½œä¸ºä¸€æ¡è®°å½•
            results.append({
                "chapter": current_chapter,
                "content": line,
                "is_chapter_title": True,
                "article_number": 0
            })
            continue

        # æ£€æµ‹æ¡æ¬¾å¼€å§‹ï¼ˆç¬¬Xæ¡ï¼‰
        article_match = re.match(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶\d]+æ¡)', line)
        if article_match:
            # ä¿å­˜å‰ä¸€æ¡æ¬¾
            if current_article and current_content:
                article_number += 1
                full_content = '\n'.join(current_content).strip()
                results.append({
                    "chapter": current_chapter,
                    "article_title": current_article,
                    "content": full_content,
                    "is_chapter_title": False,
                    "article_number": article_number
                })

            # å¼€å§‹æ–°æ¡æ¬¾
            current_article = article_match.group(1)
            current_content = [line]
        else:
            # æ™®é€šå†…å®¹è¡Œï¼Œæ·»åŠ åˆ°å½“å‰æ¡æ¬¾
            if current_content and line:
                current_content.append(line)
            elif line and not current_article:
                # ç« èŠ‚å†…çš„è¯´æ˜æ€§æ–‡å­—ï¼Œå•ç‹¬ä½œä¸ºè®°å½•
                results.append({
                    "chapter": current_chapter,
                    "article_title": "",
                    "content": line,
                    "is_chapter_title": False,
                    "article_number": 0
                })

    # å¤„ç†æœ€åä¸€æ¡æ¡æ¬¾
    if current_article and current_content:
        article_number += 1
        full_content = '\n'.join(current_content).strip()
        results.append({
            "chapter": current_chapter,
            "article_title": current_article,
            "content": full_content,
            "is_chapter_title": False,
            "article_number": article_number
        })

    return results


def process_single_file(path: Path):
    """å¤„ç†å•ä¸ªæ³•å¾‹æ–‡ä»¶"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()

        # åˆ†ç¦»å…ƒä¿¡æ¯å’Œæ³•å¾‹å†…å®¹
        parts = content.split("<!-- INFO END -->", 1)
        if len(parts) != 2:
            print(f"âš ï¸ æ–‡ä»¶æ ¼å¼é”™è¯¯: {path}")
            return []

        _, law_content = parts
        parsed_articles = parse_law_content(law_content)

        # æ·»åŠ å…ƒæ•°æ®
        folder_name = path.parent.name
        law_type = extract_type_from_folder(folder_name)
        law_name = extract_law_name_from_file(path.name)

        for article in parsed_articles:
            article.update({
                "type": law_type,
                "law": law_name,
                "source_file": path.name,
                "content_length": len(article["content"])
            })

        print(f"  ğŸ“Š åˆ‡åˆ†ä¸º {len(parsed_articles)} ä¸ªæ¡æ¬¾")
        return parsed_articles

    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {path}: {e}")
        return []


def analyze_article_distribution(data):
    """åˆ†ææ¡æ¬¾åˆ†å¸ƒ"""
    total_articles = len(data)
    chapter_titles = len([d for d in data if d.get('is_chapter_title', False)])
    actual_articles = len([d for d in data if d.get('article_number', 0) > 0])
    other_content = total_articles - chapter_titles - actual_articles

    print(f"\nğŸ“ˆ æ¡æ¬¾åˆ†å¸ƒåˆ†æ:")
    print(f"   æ€»è®°å½•æ•°: {total_articles}")
    print(f"   ç« èŠ‚æ ‡é¢˜: {chapter_titles}")
    print(f"   æ³•å¾‹æ¡æ¬¾: {actual_articles}")
    print(f"   å…¶ä»–å†…å®¹: {other_content}")

    # å†…å®¹é•¿åº¦åˆ†å¸ƒ
    lengths = [item["content_length"] for item in data]
    print(f"   å¹³å‡é•¿åº¦: {sum(lengths) / len(lengths):.0f} å­—ç¬¦")
    print(f"   æœ€å°é•¿åº¦: {min(lengths)} å­—ç¬¦")
    print(f"   æœ€å¤§é•¿åº¦: {max(lengths)} å­—ç¬¦")

    # é•¿åº¦åˆ†å¸ƒ
    dist = {
        "å°äº100": len([l for l in lengths if l < 100]),
        "100-300": len([l for l in lengths if 100 <= l < 300]),
        "300-500": len([l for l in lengths if 300 <= l < 500]),
        "å¤§äº500": len([l for l in lengths if l >= 500])
    }

    print("   é•¿åº¦åˆ†å¸ƒ:")
    for range_name, count in dist.items():
        percentage = (count / total_articles) * 100
        print(f"     {range_name}: {count} æ¡ ({percentage:.1f}%)")


def process_all_files(raw_dir: Path = DATA_RAW_DIR,
                      output_json: Path = DATA_PROCESSED_DIR / "laws_processed.json"):
    """å¤„ç†æ‰€æœ‰æ³•å¾‹æ–‡ä»¶"""

    md_files = list(raw_dir.rglob("*.md"))
    print(f"ğŸ“„ æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")

    if not md_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°Markdownæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ data/raw ç›®å½•")
        return None

    all_data = []
    processed_files = 0

    for md_file in md_files:
        print(f"\nâ¡ï¸ å¤„ç† {md_file.relative_to(raw_dir)}")
        file_data = process_single_file(md_file)
        all_data.extend(file_data)
        processed_files += 1
        print(f"  âœ… å·²å®Œæˆ {processed_files}/{len(md_files)} æ–‡ä»¶")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_json.parent.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å¤„ç†ç»“æœ
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"   å¤„ç†æ–‡ä»¶: {processed_files} ä¸ª")
    print(f"   ç”Ÿæˆæ¡æ¬¾: {len(all_data)} æ¡")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_json}")

    # åˆ†ææ¡æ¬¾åˆ†å¸ƒ
    analyze_article_distribution(all_data)

    return output_json


if __name__ == "__main__":
    process_all_files()